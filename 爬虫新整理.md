

# request模块

## 百度翻译

```
import requests
# 爬取网站：https://fanyi.baidu.com
url = "https://fanyi.baidu.com/sug"

s = input("请输入要翻译的英文单词")
dat = {
    "kw": s
}

# 发送post请求，发送的数据必须放在字典中，通过data参数进行传递
resp = requests.post(url, data=dat)
print(resp.json())
```

![image-20220218114238737](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218114238737.png)

![image-20220218114315053](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218114315053.png)

百度翻译在搜索时，sug里面的kw会发生变化，我们定义一个字典来改变这里的值

代码输出：

![image-20220218114658157](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218114658157.png)

## 豆瓣排行榜

数据在没有在源代码中，需要js加载

![image-20220218193716959](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218193716959.png)

其中js就在下方的网址中

```
Request URL: https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20
```

![image-20220218193746721](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218193746721.png)

并且其中的封装的参数如下

![image-20220218193849127](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218193849127.png)

其中，每一次加载数据，只有limit在更新

![image-20220218194045471](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218194045471.png)

```
import requests

# 爬取网站 https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&type=24&interval_id=100:90&action=
# 需要加载的js地址：https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20
# #这个连接中？后的为数据
url = "https://movie.douban.com/j/chart/top_list"



# 重新封装参数
param = {
    "type": 24,
    "interval_id": "100:90",
    "action": "",
    "start": 0,
    "limit": 20
}

# resp = requests.get(url=url, params=param)
#
# # 输出地址
# print(resp.request.url)
#
# print(resp.request.headers)

# #重新定义User-Agent
header = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3878.400 QQBrowser/10.8.4518.400"
}
resp = requests.get(url=url, params=param, headers=header)
print(resp.text)
print(resp.json())
resp.close()   # 关掉resp

```

# 关于代码中数据的提取三种方式

## 正则re模块

![image-20220219193822696](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193822696.png)

量词

![image-20220219193840026](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193840026.png)

![image-20220219193921517](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193921517.png)

![image-20220219193928957](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193928957.png)

### 使用正则re爬取豆瓣排行榜

数据在源代码中

![image-20220218195007459](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218195007459.png)

```
r'<li>.*?<div class="item">.*?<span class="title">(?P<name>.*?)</span>'  #获取电影名
```

![image-20220218195042288](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218195042288.png)

```
r'.*?<p class="">.*?<br>(?P<year>.*?)&nbsp'                              #获取电影上映年份
```

```
import requests
import re

#存如csv格式
import csv

url = "https://movie.douban.com/top250"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3878.400 QQBrowser/10.8.4518.400"
}
resp = requests.get(url, headers=headers)
page_content = resp.text

#解析数据
obj = re.compile(r'<li>.*?<div class="item">.*?<span class="title">(?P<name>.*?)</span>'  #获取电影名
                 r'.*?<p class="">.*?<br>(?P<year>.*?)&nbsp'                              #获取电影上映年份
                 r'.*?<span class="rating_num" property="v:average">(?P<score>.*?)</span>' #获取电影评分
                 r'.*?<span>(?P<num>.*?)人评价</span>', re.S)
#开始匹配
result = obj.finditer(page_content)
f = open("data.csv", mode="w", encoding="utf-8")
csvwriter = csv.writer(f)

#下面这个循环不注释掉，最后面的循环会出错
# for it in result:
#     print(it.group("name"))
#     print(it.group("year").strip())    #去掉name和year之间的间隔
#     print(it.group("score"))
#     print(it.group("num"))

for i in result:
    dic = i.groupdict()
    dic['year'] = dic['year'].strip()
    csvwriter.writerow(dic.values())
f.close()
resp.close()
print("Over!")

```

### 屠戮盗版天堂

数据在源代码中

1、定位到2020必看片

2、从2020必看片中提取到子页面的链接地址

3、请求子页面的链接地址，拿到我们想要的下载地址。。。

![image-20220218195828204](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218195828204.png)

![image-20220218195959709](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218195959709.png)

```
domain = "https://dytt89.com/"
resp = requests.get(domain, verify=False)  #verify=False：不做访问页面时的校验
resp.encoding = 'gb2312'  #指定字符集
#拿到ul里面的li
obj1 = re.compile(r'2022必看热片.*?<ul>(?P<ul>.*?)</ul>', re.S)
result1 = obj1.finditer(resp.text)
```

2、找到子页面的地址

![image-20220218200036501](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218200036501.png)

```
obj2 = re.compile(r"<a href='(?P<href>.*?)'", re.S)
child_href_list = []   #创建一个列表
for it in result1:
    ul = it.group('ul')
    # print(ul)
    # 提取子页面链接
    result2 = obj2.finditer(ul)
    for itt in result2:
        # print(itt.group('href'))
        #本句是将https://dytt89.com/ + /i/104951.html 将这个链接拼接起来
        child_href = domain + itt.group('href').strip("/")   #.strip("/")是将链接中多余的一个/去掉
        child_href_list.append(child_href)   #把子页面保存起来
```

3、定位到子页面中的下载地址

![image-20220218200106172](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218200106172.png)

![image-20220218200113228](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218200113228.png)

```
obj3 = re.compile(r'◎片　　名(?P<movie>.*?)<br />'
                  r'.*?<td style="WORD-WRAP: break-word" bgcolor="#fdfddf"><a href="(?P<download>.*?)">', re.S)
                  for href in child_href_list:
    child_resp = requests.get(href, verify=False)
    child_resp.encoding = 'gbk'
    result3 = obj3.search(child_resp.text)
    print(result3.group("movie"))
    print(result3.group("download"))
    break
```

代码：

```
import requests
import re
import time

domain = "https://dytt89.com/"
resp = requests.get(domain, verify=False)  #verify=False：不做访问页面时的校验
resp.encoding = 'gb2312'  #指定字符集
# print(resp.text)

#拿到ul里面的li
obj1 = re.compile(r'2022必看热片.*?<ul>(?P<ul>.*?)</ul>', re.S)
obj2 = re.compile(r"<a href='(?P<href>.*?)'", re.S)
obj3 = re.compile(r'◎片　　名(?P<movie>.*?)<br />'
                  r'.*?<td style="WORD-WRAP: break-word" bgcolor="#fdfddf"><a href="(?P<download>.*?)">', re.S)

result1 = obj1.finditer(resp.text)
child_href_list = []   #创建一个列表
for it in result1:
    ul = it.group('ul')
    # print(ul)
    # 提取子页面链接
    result2 = obj2.finditer(ul)
    for itt in result2:
        # print(itt.group('href'))
        #本句是将https://dytt89.com/ + /i/104951.html 将这个链接拼接起来
        child_href = domain + itt.group('href').strip("/")   #.strip("/")是将链接中多余的一个/去掉
        child_href_list.append(child_href)   #把子页面保存起来

#提取子页面内容
for href in child_href_list:
    child_resp = requests.get(href, verify=False)
    child_resp.encoding = 'gbk'
    result3 = obj3.search(child_resp.text)
    print(result3.group("movie"))
    print(result3.group("download"))
    time.sleep(1)
    # break
```

## bs4模块

### 菜价的整理

数据在源代码中

```
爬取网站     https://www.construdip.com/marketanalysis/0/list/1.shtml
```

![image-20220218200939095](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218200939095.png)

![image-20220218200945720](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218200945720.png)

```
import requests
from bs4 import BeautifulSoup
import csv

url = "https://www.construdip.com/marketanalysis/0/list/1.shtml"
resp = requests.get(url)
# print(resp.text)

#解析数据
#1、把页面源代码交给BeautifulSoup进行处理，生成bs对象
page = BeautifulSoup(resp.text, "html.parser")   #指定html解析器
#2、从bs对象中查找数据
# find(标签，属性=值)
# find_all(标签，属性=值)
# table = page.find("table", class_="hq_table")  #class是python中的关键字
table = page.find("table", attrs={"class": "hq_table"})  #和上面的语句相同，使用字典的形式
# print(table)

#建立一个文件
f = open("菜价.csv", mode="w", encoding="utf-8")
csvwriter = csv.writer(f)

#拿到所有数据行
trs = table.find_all("tr")[1:]
for tr in trs:   #每一行
    tds = tr.find_all("td")   #拿到每一行中的所有td
    name = tds[0].text       #.text 表示拿到被标签标记的内容
    low = tds[1].text
    avg = tds[2].text
    high = tds[3].text
    gui = tds[4].text
    kind = tds[5].text
    date = tds[6].text
    csvwriter.writerow([name, low, avg, high, gui, kind, date])
f.close()
print("Over")
```

### 美图网站图片下载

图片下载地址在源代码中

对下载地址位置进行缩小，先从主页面依次获得子页面的地址

![image-20220218202211930](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218202211930.png)

```
main_page = BeautifulSoup(resp.text, "html.parser")
alist = main_page.find("div", class_="TypeList").find_all("a")   #把范围第一次缩小
```

在循环中，依次进入子页面，拿到子页面的下载地址，进入后获得图片的下载地址

```
href = url + a.get('href').strip("bizhitupian/weimeibizhi/")+"htm" #strip()是将有重复的这个内容去掉
```

定位到图片下载地址

![image-20220218202945635](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218202945635.png)

```
# 拿到子页面的源代码
    child_page_resp = requests.get(href)
    child_page_resp.encoding = "utf-8"
    child_page_text = child_page_resp.text
    # 从子页面中拿到图片的x下载路径
    child_page = BeautifulSoup(child_page_text, "html.parser")
    div = child_page.find("div", class_="ImageBody")
    img = div.find("img")  #把范围缩小到img的属性中
    # print(img.get("src"))
    src = img.get("src")  #拿到网页中src中的属性
```

全部代码：

```
# 1、拿到主页面的源代码，然后提取到子页面的链接地址，href
# 2、通过href拿到子页面的内容，从子页面中找到图片的下载地址  img->src
# 3、下载图片
import requests
from bs4 import BeautifulSoup
import time  #防止频繁访问服务器被封号

url = "https://umei.net/bizhitupian/weimeibizhi/"
resp = requests.get(url)
resp.encoding = 'utf-8'
# print(resp.text)

#把源代码交给bs
main_page = BeautifulSoup(resp.text, "html.parser")
alist = main_page.find("div", class_="TypeList").find_all("a")   #把范围第一次缩小
# print(alist)
for a in alist:
    # print(url + a.get('href').strip("bizhitupian/weimeibizhi/")+"htm")  #直接通过get就可以拿到属性的值
    href = url + a.get('href').strip("bizhitupian/weimeibizhi/")+"htm"
    # print(href)
    # 拿到子页面的源代码
    child_page_resp = requests.get(href)
    child_page_resp.encoding = "utf-8"
    child_page_text = child_page_resp.text
    # 从子页面中拿到图片的x下载路径
    child_page = BeautifulSoup(child_page_text, "html.parser")
    div = child_page.find("div", class_="ImageBody")
    img = div.find("img")  #把范围缩小到img的属性中
    # print(img.get("src"))
    src = img.get("src")  #拿到网页中src中的属性
    #下载图片
    img_resp =requests.get(src)
    img_name = src.split("/")[-1]   #拿到url中的最后一个/之后的内容
    with open("img/"+ img_name, mode="wb") as f:    #加上这个"img/"+ 就开始报错了；必须手动建立一个img文件夹
    # with open("img/" + img_name, mode="wb") as f:
        f.write(img_resp.content)  #图片内容写入文件；img_resp.content 拿到的是图片的字节

    print("Over!", img_name)
    time.sleep(1)
f.close()
child_page_resp.close()
print("All Over!!")
```

## xpath模块

xpath对数据的几种处理方式

```
from lxml import etree
tree = etree.XML(xml)   #表示拿取为xml文件数据
tree = etree.parse("a.html", etree.HTMLParser()) #表示拿取数据为当地文件类型
html = etree.HTML(resp.text)    #表示数据为html文件类型

result1 = tree.xpath("/book")  # /表示层级关系，第一个/是根节点
result2 = tree.xpath("/book/name/text()")  #text()拿文本
result3 = tree.xpath("/book/author//nick/text()")  #拿取author中的所有nick；包含div中的nick
print(result3)
result4 = tree.xpath("/book/author/*/nick/text()")  #* 任意的节点，通配符
result2 = tree.xpath("/html/body/ol/li/a[@href='dapao']/text()")  #[@xxx=xxx] 属性的筛选，关于herf为dapao的标签
```

### 对周八戒网站数据获取

数据在源代码中

获取网站中商铺的信息

![image-20220219200209415](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219200209415.png)

```
from lxml import etree

import requests

url = "https://beijing.zbj.com/search/f/?type=new&kw=saas"
resp = requests.get(url)
# print(resp.text)

#解析
html = etree.HTML(resp.text)
divs = html.xpath("/html/body/div[6]/div/div/div[2]/div[5]/div[1]/div")
for div in divs:
    price = div.xpath("./div/div/a/div[2]/div[1]/span[1]/text()")[0].strip("¥")
    title = "saas".join(div.xpath("./div/div/a/div[2]/div[2]/p/text()"))   #连接标题，“saas”是搜索的字符串
    com_name = div.xpath("./div/div/a/div[1]/p/text()")
    # "//*[@id="utopia_widget_76"]/a[1]/div[1]/p/text()"
    location = div.xpath("./div/div/a[2]/div[1]/div/span/text()")
    print(com_name)
```

# 简单的几种反爬方式

```
1、user-Agent: 请求载体的身份标识
2、Referer:  防盗链（这次的请求是从哪个页面来的？）
3、Cookie:   本地字符串数据信息（用户登录信息，反爬的token）
4、Accept：这是服务器返回内容的简要信息
5、Form Data：这是我们传递给服务器的内容，可以发现，这就是我们填写的需翻译内容，以及翻译的源语种和目标语种
```



## 处理cookie

```
# session可以认为是一连串的请求，在这个过程中cookie不会丢
# 会话
session = requests.session()
```

### 17k小说网模拟用户登录

数据没有在源代码中，需要js加载

```
爬取网站   https://user.17k.com/www/bookshelf/
```

1、登录 -> 得到cookie

2、带着cookie 去请求到书架url -> 书架上的内容

3、必须把上面的内容连接起来

4、我们可以使用session进行请求 -> session可以认为是一连串的请求，在这个过程中的cookie不会丢失

在我们登录自己的账户后看到的内容

![image-20220218212600149](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218212600149.png)

![image-20220218212704196](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218212704196.png)



登录之后我们找到书架的书籍内容

![image-20220218213202370](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218213202370.png)

![image-20220218213319128](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218213319128.png)

```
import requests
# session可以认为是一连串的请求，在这个过程中cookie不会丢
# 会话
session = requests.session()
data = {
    "loginName": "你的账户",  #你的账户
    "password": "你的密码"	#你的密码
}

#1、登录
url = "https://passport.17k.com/ck/user/login"
# resp = session.post(url, data=data)
session.post(url, data=data)
# print(resp.text)
# print(resp.cookies)

#2、拿书架上的数据
resp = session.get('https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919')
# print(resp.text)
print(resp.json())
```

![image-20220218213439071](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218213439071.png)

## 防盗链Referer的处理

### 梨视频网站视频的下载

```
爬取网站   https://www.pearvideo.com/
```

![image-20220218215141780](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218215141780.png)

这里的视频地址是可以打开播放的，但是下面的地址是不可以的

`提取的视频地址是可以播放的https://video.pearvideo.com/mp4/short/20220209/cont-1681528-15824270-hd.mp4`
`但是这里的视频地址确实不可以播放的https://video.pearvideo.com/mp4/short/20220209/1644561324854-15824270-hd.mp4`

![image-20220218215253541](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218215253541.png)

通过比较发现是cont-1681528和1644561324854不一致导致的

![image-20220218215434767](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218215434767.png)

![image-20220218215615740](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218215615740.png)

![image-20220218215641845](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220218215641845.png)

获取地址

```
resp = requests.get(videoStatusURL, headers=headers)
print(resp.json())
结果为
	{'resultCode': '1', 'resultMsg': 'success', 'reqId': '6b3d219b-c84a-4c4b-90bb-708fa603cc65', 'systemTime': '1645193588353', 'videoInfo': {'playSta': '1', 'video_image': 'https://image.pearvideo.com/cont/20220209/cont-1681528-12647637.png', 'videos': {'hdUrl': '', 'hdflvUrl': '', 'sdUrl': '', 'sdflvUrl': '', 'srcUrl': 'https://video.pearvideo.com/mp4/short/20220209/1645193588353-15824270-hd.mp4'}}}
	
srcUrl = dic['videoInfo']['videos']['srcUrl']  #拿取到videoInfo中的videos里面的地址srcUrl
print(srcUrl)
结果如下：
	https://video.pearvideo.com/mp4/short/20220209/1645193588353-15824270-hd.mp4
```

得到正确视频下载地址

```
systemTime = dic['systemTime']  #拿取到systemTime的值，值是1645192834127，要把这个值是1645192834127替换为cont-1681528
srcUrl = srcUrl.replace(systemTime, f"cont-{contID}")
```

全部代码

```
#1、拿到contID
#2、拿到videoStatus返回的json. -> srcURL
#3、srcURL 里面的内容进行修整
#4、下载视频

import requests
url = "https://www.pearvideo.com/video_1681528"
contID = url.split("_")[1]
# print(contID)

#要把这个地址里面的内容进行修改
videoStatusURL = f"https://www.pearvideo.com/videoStatus.jsp?contId={contID}&mrd=0.29587422804361174"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3878.400 QQBrowser/10.8.4518.400",
    # 防盗链
    "Referer": "https://www.pearvideo.com/video_1681528"
}
resp = requests.get(videoStatusURL, headers=headers)
# print(resp.json())
# resp.json()['videoInfo']['videos']['srcUrl']
dic = resp.json()
srcUrl = dic['videoInfo']['videos']['srcUrl']
# print(srcUrl)
systemTime = dic['systemTime']
srcUrl = srcUrl.replace(systemTime, f"cont-{contID}")
# print(srcUrl)

#下载视频
with open("a.mp4", mode="wb") as f:
    f.write(requests.get(srcUrl).content)
```

## IP代理

```
import requests

url = "https://www.baidu.com"
#代理ip
proxies = {
    "https://": "https://211.136.128.154:53281"
}

resp = requests.get(url, proxies=proxies)
resp.encoding = "utf-8"
print(resp.text)
```

# 几种下载方式

多线程和多进程的格式

```
多线程
#target=func 不加括号是对函数的调用；target=func()加括号是对结果的调用
    t = Thread(target=func)  # 创建一个多线程对象
    t.start()   #多线程为可以开始工作状态
 方式二：
 t = MyThread()  #定义一个对象
 t.start()       #开启线程
 class MyThread(Thread):   #子类继承了Thread
    def run(self):        #这里的方法必须是run()
    
多进程
#target=func 表示是对函数的调用；target=func()表示是对结果的调用
    p = Process(target=func)
    p.start()  # 开启多进程
 传参格式
    p1 = Process(target=func, args=("周杰伦",))  #传递参数必须是元组
    p1.start()  # 开启多进程
```



## 多线程

### demo1

```
#多线程
from threading import Thread

def func():
     for i in range(100):
         print("func", i)

if __name__ == '__main__':
    #target=func 不加括号是对函数的调用；target=func()加括号是对结果的调用
    t = Thread(target=func)  # 创建一个多线程对象
    t.start()   #多线程为可以开始工作状态
    for i in range(100):
        print("main", i)
```

### demo2

```
from threading import Thread

class MyThread(Thread):   #子类继承了Thread
    def run(self):        #这里的方法必须是run()
        for i in range(1000):
            print("子线程", i)

if __name__ == '__main__':
    t = MyThread()  #定义一个对象
    t.start()       #开启线程
    for i in range(1000):
        print("主线程", i)
```



## 多进程

### demo1

```
from multiprocessing import Process

def func():
    for i in range(10000):
        print("子进程", i)

if __name__ == '__main__':
    #target=func 表示是对函数的调用；target=func()表示是对结果的调用
    p = Process(target=func)
    p.start()  # 开启多进程
    for i in range(10000):
        print("主进程", i)
```

### demo2

```
from multiprocessing import Process

def func(name):
    for i in range(10000):
        print(name, i)

if __name__ == '__main__':
    #target=func 表示是对函数的调用；target=func()表示是对结果的调用
    p1 = Process(target=func, args=("周杰伦",))  #传递参数必须是元组
    p1.start()  # 开启多进程
    p2 = Process(target=func, args=("王力宏",))  # 传递参数必须是元组
    p2.start()  # 开启多进程
```

## 线程池和进程池

### demo

```
#导入线程池和进程池
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

def fn(name):
    for i in range(10):
        print(name, i)
if __name__ == '__main__':
    #创建线程池
    with ThreadPoolExecutor(50) as t:
        for i in range(10):
            t.submit(fn, name=f"线程{i}")   #调度任务
    #等待线程池中的任务全部执行完毕，才继续执行（守护）
    print("完成")
```

### xpath+爬取菜价

数据在页面源代码中

```
爬取地址   https://www.construdip.com/marketanalysis/0/list/1.shtml
```

![image-20220219182631488](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219182631488.png)

定位到数据的位置

```
def download_one_page(url):
    resp = requests.get(url)
    # print(resp.text)
    html = etree.HTML(resp.text)
    #下面要是不加[0]就是一个列表，且只有一个元素，索引为0
    table = html.xpath("/html/body/div[2]/div[4]/div[1]/table")[0]
    # print(table)
```

去除第一行的标签

![image-20220219182726771](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219182726771.png)

```
 # [1:]每页获取的菜价还有一个 “菜名”  “价格” 之类的东西，[1:]就是用来去除列表第一行元素的
    # trs = table.xpath("./tr")[1:]
    trs = table.xpath("./tr[position()>1]")  #方法二，舍弃第一行的数据
    # print(len(trs))
```

![image-20220219182806555](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219182806555.png)

```
#拿到每个tr
    for tr in trs:
        txt = tr.xpath("./td/text()")
        print(txt)
输出结果为
['大白菜', '0.45', '0.48', '0.50', '\\新', '斤', '2021-07-16']
['娃娃菜', '0.50', '0.60', '0.70', '大\\冀新', '斤', '2021-07-16']
['小白菜', '1.40', '1.55', '1.70', '普通', '斤', '2021-07-16']
['芹菜', '1.00', '1.10', '1.20', '\\冀', '斤', '2021-07-16']
....
```

```
#对数据做简单的处理：\\  /去掉
        txt = (item.replace("\\","").replace("/", "") for item in txt)
        print(list(txt))
输出结果
['大白菜', '0.45', '0.48', '0.50', '新', '斤', '2021-07-16']
['娃娃菜', '0.50', '0.60', '0.70', '大冀新', '斤', '2021-07-16']
['小白菜', '1.40', '1.55', '1.70', '普通', '斤', '2021-07-16']
。。。。
```

程序代码

```
import requests
from lxml import etree
import csv
from concurrent.futures import ThreadPoolExecutor

f = open("线程池_菜价.csv", mode="w", encoding="utf-8")
csvwriter = csv.writer(f)

def download_one_page(url):
    resp = requests.get(url)
    # print(resp.text)
    html = etree.HTML(resp.text)
    #下面要是不加[0]就是一个列表，且只有一个元素，索引为0
    table = html.xpath("/html/body/div[2]/div[4]/div[1]/table")[0]
    # print(table)
    # [1:]每页获取的菜价还有一个 “菜名”  “价格” 之类的东西，[1:]就是用来去除列表第一行元素的
    # trs = table.xpath("./tr")[1:]
    trs = table.xpath("./tr[position()>1]")  #方法二，舍弃第一行的数据
    # print(len(trs))
    #拿到每个tr
    for tr in trs:
        txt = tr.xpath("./td/text()")
        # print(txt)
        #对数据做简单的处理：\\  /去掉
        txt = (item.replace("\\","").replace("/", "") for item in txt)
        # print(list(txt))
        csvwriter.writerow(txt)
    print(url, "提取完毕")

if __name__ == '__main__':
    # download_one_page("https://www.construdip.com/marketanalysis/0/list/1.shtml")

    #单线程的下载方式
    # for i in range(1, 100):
    #     download_one_page(f"https://www.construdip.com/marketanalysis/0/list/{i}.shtml")
    with ThreadPoolExecutor(50) as t:
        for i in range(1, 200):
            #把下载任务提交给线程池
            t.submit(download_one_page, f"https://www.construdip.com/marketanalysis/0/list/{i}.shtml")
```

## 协程

```
方法一：
async def func1():
    print("程序1")
    # time.sleep(3) #当程序出现了同步操作的时候，异步就中断了
    await asyncio.sleep(3)  #异步操作的代码;await表示挂起，cpu执行下一个程序
    print("程序1")
f1 = func1()
f2 = func2()
f3 = func3()
#把任务放到列表里面去
tasks = [f1, f2, f3]
#一次性启动多个任务(协程)
asyncio.run(asyncio.wait(tasks))

方法二:
async def main():
    #第一种写法
    # f1 = func1()
    # await f1   #一般await挂起操作放在协程对象前面
    #第二种写法
    tasks = [asyncio.create_task(func1()), asyncio.create_task(func2()), asyncio.create_task(func3())]
    await asyncio.wait(tasks)
if __name__ == '__main__':
    asyncio.run(main())
```

### aiohttp模块应用下载图片

```
import asyncio
import aiohttp

urls = [
    "http://kr.shanghai-jiuxin.com/file/2020/1031/small774218be86d832f359637ab120eba52d.jpg",
    "http://kr.shanghai-jiuxin.com/file/2020/1031/small563337d07af599a9ea64e620729f367e.jpg",
    "http://kr.shanghai-jiuxin.com/file/2020/1031/small26b7e178e987be6d914bf8d1af120890.jpg"
]
async def aiodownload(url):
    name = url.rsplit("/", 1)[1]   #创建文件名为最后一个
    #这里加了with就相当于开文件时的一样，执行完这里之后就不用手动进行释放了
    #aiohttp.ClientSession() <==> 相当于 requests模块
    async with aiohttp.ClientSession() as session:  #async表示异步
        async with session.get(url) as resp:
            # resp.content.read()    #<==>等价于requests模块中的 resp.content
            #请求回来了，写入文件
            with open(name, mode="wb") as f:
                f.write(await resp.content.read())  #读取内容是异步的，需要await挂起
    print(name, "搞定")

async def main():
    tasks = []
    for url in urls:
        tasks.append(asyncio.create_task(aiodownload(url)))
        # tasks.append(aiodownload(url))
    await asyncio.wait(tasks)


if __name__ == '__main__':
    # asyncio.run(main())  #这里的会报错
    asyncio.get_event_loop().run_until_complete(main())
```

### 用协程爬取一部小说

```
爬取网站： http://dushu.baidu.com/pc/detail?gid=4306063500
```

![image-20220219184247586](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184247586.png)

点击页面的查看更多后出现

![image-20220219184305535](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184305535.png)

此时，http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"4306063500"}

第一章的小说在这里

![image-20220219184339281](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184339281.png)

![image-20220219184356844](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184356844.png)

```
http://dushu.baidu.com/api/pc/getChapterContent?data={%22book_id%22:%224306063500%22,%22cid%22:%224306063500|1569782244%22,%22need_bookinfo%22:1}
```

其中的%22为符号""号，所以，修改后的地址为

```
http://dushu.baidu.com/api/pc/getChapterContent?data={"book_id":"4306063500","cid":"4306063500|1569782244","need_bookinfo":1}
```

![image-20220219184437845](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184437845.png)

```
# url = 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":f"{b_id}"}'  这样写就会报错，只能用下面的
    url = 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"' + b_id + '"}'
```

![image-20220219184601003](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219184601003.png)

代码：

```
import requests
import asyncio
import aiohttp
import json
import aiofiles

#1、同步操作：访问getCatalog ，拿到所有章节的cid和名称
#2、异步操作：访问 getChapterContent，下载所有的文章内容
#
# "http://dushu.baidu.com/pc/detail?gid=4306063500"
#
# 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"4306063500"}'
async def aiodownload(cid, b_id, title):
    data = {
        "book_id": f"{b_id}",
        "cid": f"{b_id}|{cid}",
        "need_bookinfo": 1
    }
    data = json.dumps(data)   #将data转换为字符串
    url = f"http://dushu.baidu.com/api/pc/getChapterContent?data={data}"

    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            dic = await resp.json()

            async with aiofiles.open("西游记小说/"+title, mode="w", encoding="utf-8") as f:
                await f.write(dic['data']['novel']['content'])


async def getCatalog(url):
    resp = requests.get(url)
    # print(resp.text)
    # print(resp.json())
    dic = resp.json()
    tasks = []
    # 在data里面找到novel，再在novel中找到items；
    for item in dic['data']['novel']['items']:
        title = item['title']  #在items中找到title
        cid = item['cid']
        #准备异步任务
        tasks.append(asyncio.create_task(aiodownload(cid, b_id, title)))
        # print(title, cid)
    await asyncio.wait(tasks)

if __name__ == '__main__':
    b_id = "4306063500"
    # url = 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":f"{b_id}"}'  这样写就会报错，只能用下面的
    url = 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"' + b_id + '"}'
    asyncio.run(getCatalog(url))
```

# selenium模块

```
环境搭建：
	下载浏览器驱动  
		https://registry.npmmirror.com/binary.html?path=chromedriver/
```

将下载好的文件解压后放到相应的文件中，例如

![image-20220219185451370](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219185451370.png)

![image-20220219185916150](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219185916150.png)

```
from selenium.webdriver import Chrome
web = Chrome()   #面向谷歌浏览器建立一个对象
web.get("http://www.baidu.com")
```

## 对招聘网站进行职位搜索

```
操作网站： https://www.lagou.com/
```

打开网站

```
web = Chrome()
web.get("http://lagou.com")
```

![image-20220219190231916](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219190231916.png)

点击弹窗中的x，进入主页面

```
#找到某个元素，点击它
# el = web.find_element_by_xpath('//*[@id="changeCityBox"]/p[1]/a')  #旧版本的，新版本如下
el = web.find_element(By.XPATH, '//*[@id="changeCityBox"]/p[1]/a')
el.click()
```

在搜索框中输入python,按下回车进行搜索

```
time.sleep(2)
# 找到输入框，输入python ==> 输入回车/点击搜索按钮
web.find_element(By.XPATH, '//*[@id="search_input"]').send_keys("python", Keys.ENTER)
```

![image-20220219190240894](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219190240894.png)

![image-20220219190257231](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219190257231.png)

代码：

```
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time
web = Chrome()
web.get("http://lagou.com")

#找到某个元素，点击它
# el = web.find_element_by_xpath('//*[@id="changeCityBox"]/p[1]/a')  #旧版本的，新版本如下
el = web.find_element(By.XPATH, '//*[@id="changeCityBox"]/p[1]/a')
el.click()

time.sleep(2)
# 找到输入框，输入python ==> 输入回车/点击搜索按钮
web.find_element(By.XPATH, '//*[@id="search_input"]').send_keys("python", Keys.ENTER)

# 查找存放数据的位置，进行数据提取
# 找到页面中存放数据的所有li
li_list = web.find_elements(By.XPATH, '//*[@id="jobList"]/div[1]/div')
for li in li_list:
    job_name = li.find_element(By.TAG_NAME, "a").text
    job_price = li.find_element(By.XPATH, './div/div/div[2]/span').text
    print(job_price, job_name)
    time.sleep(1)
```

## 招聘网站窗口切换

```
from selenium.webdriver import Chrome
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
web = Chrome()
web.get("http://lagou.com")
#关闭弹窗
web.find_element(By.XPATH, '//*[@id="cboxClose"]').click()
time.sleep(1)

#搜索栏搜索python,并进行回车操作
web.find_element(By.XPATH, '//*[@id="search_input"]').send_keys("python", Keys.ENTER)
time.sleep(1)

#点击进入当前岗位，会打开一个新的页面
web.find_element(By.XPATH, '//*[@id="jobList"]/div[1]/div[1]/div[1]/div[1]/div[1]/a').click()

#如何进入到新窗口中进行提取
web.switch_to.window(web.window_handles[-1])

#在新窗口中提取内容
job_detil = web.find_element(By.XPATH, '//*[@id="job_detail"]/dd[2]/div/p').text
print(job_detil)

#关闭子窗口
web.close()
#变更selenium的窗口视角，回到原来的窗口中
web.switch_to.window(web.window_handles[0])

```

## 让浏览器在后台操作

```
#准备好参数配置
opt = Options()
opt.add_argument("--headless")  #无头操作
opt.add_argument("--disable-gpu")  #不让gpu渲染

web = Chrome(options=opt)
```

代码：

```
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
#引入下拉选择的包
from selenium.webdriver.support.select import Select
import time
#浏览器在后台操作
from selenium.webdriver.chrome.options import Options

#准备好参数配置
opt = Options()
opt.add_argument("--headless")  #无头操作
opt.add_argument("--disable-gpu")  #不让gpu渲染


web = Chrome(options=opt)
web.get("https://www.endata.com.cn/BoxOffice/BO/Year/index.html")

#定位到下拉页表
sel_el = web.find_element(By.XPATH, '//*[@id="OptionDate"]')
#对元素进行包装，包装成下拉菜单
sel = Select(sel_el)

#让浏览器进行调整选项
for i in range(len(sel.options)):
    sel.select_by_index(i)   #按照索引进行切换
    time.sleep(2)
    table = web.find_element(By.XPATH, '//*[@id="TableList"]/table')
    print(table.text)
    print("===================================================")
print("运行完毕")
web.close()

# 如何拿到页面代码Elements(经过数据加载以及js执行之后的结果的html内容)
print(web.page_source)
```

## 验证码操作

```
操作网站： http://www.chaojiying.com/user/login/
```

![image-20220219192916367](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219192916367.png)

使用超级鹰进行对验证码的解析

![image-20220219193154206](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193154206.png)

对验证码进行解析

```
img = web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/div/img').screenshot_as_png
chaojiying = Chaojiying_Client('登录账户名', '登录密码', '929057')  #最后一个是软件ID号
```

代码：

```
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from chaojiying import Chaojiying_Client
import time
web = Chrome()
web.get("http://www.chaojiying.com/user/login/")

#处理验证码
img = web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/div/img').screenshot_as_png
chaojiying = Chaojiying_Client('登录账户名', '登录密码', '929057')
dic = chaojiying.PostPic(img, 1902)  #1902
verify_code = dic['pic_str']

#向页面中填入用户名，密码，验证码
web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/p[1]/input').send_keys("登录账户名")
web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/p[2]/input').send_keys("登录密码")
web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/p[3]/input').send_keys(verify_code)
time.sleep(5)
#点击登录
web.find_element(By.XPATH, '/html/body/div[3]/div/div[3]/div[1]/form/p[4]/input').click()

```



```
其中dic = chaojiying.PostPic(img, 1902)  #1902是根据下面的内容决定的，我们验证的是四位数字字母组合的验证码，开发文档就为这种，其它验证码，根据开发手册说明决定
```

![image-20220219193553092](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220219193553092.png)

# 网易云

![image-20220306213402429](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220306213402429.png)

![image-20220306213826932](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220306213826932.png)

![image-20220306214203352](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220306214203352.png)



![image-20220307211236739](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220307211236739.png)

![image-20220307211534823](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220307211534823.png)

![image-20220307211708790](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220307211708790.png)

![image-20220307212228324](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220307212228324.png)

